{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROGRAMACIÓN I\n",
    "\n",
    "Nombre: Madeleine Yameli Romero Rayas\n",
    "\n",
    "Código: 207549458\n",
    "\n",
    "Maestría en Ciencia de los Datos (CUCEA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge de Fin de Curso: El Gran Analizador de Texto Python (GATPy):\n",
    "\n",
    "### Descripción del Challenge\n",
    "Crearás un script de Python denominado GATPy. Este script leerá un archivo de texto provisto por el usuario (por ejemplo, un libro de dominio público de Project Gutenberg). Luego, calculará y presentará lo siguiente:\n",
    "* La frecuencia de cada palabra en el texto.\n",
    "* Las 5 palabras más comunes y su conteo.\n",
    "* La cantidad de palabras únicas en el documento.\n",
    "* Opcional: Una nube de palabras generada a partir del texto.\n",
    "\n",
    "Además, el script generará un archivo de salida .txt con los resultados del análisis y una visualización simple si optas por la funcionalidad opcional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busca y elige un libro de Project Gutenberg para analizar, ingresa a https://www.gutenberg.org/ \n",
      "El libro seleccionado se encuentra en: https://www.gutenberg.org/files/2000/2000-0.txt\n",
      "Este libro tiene 24162 palabras únicas\n",
      "Las 5 palabras más comúnes en el libro son las siguientes: [('que', 20739), ('de', 18409), ('y', 18250), ('la', 10489), ('a', 9926)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x217f4937400>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos los módulos necesarios\n",
    "import requests\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Defininimos una variable que contenga los signos de puntuación\n",
    "PUNTUACION = string.punctuation + \"¿¡\"\n",
    "\n",
    "# Solicitamos al usuario el URL del libro de Project Gutenberg\n",
    "print(\"Busca y elige un libro de Project Gutenberg para analizar, ingresa a https://www.gutenberg.org/ \")\n",
    "url = input(\"Ingresa el URL del libro de Project Gutenber que quieras analizar: \")\n",
    "# Validamos que el URL sea válido\n",
    "while not urlparse(url).scheme or not urlparse(url).netloc:\n",
    "    print(\"El URL que ingresaste no es válido. Por favor, ingresa un URL que empiece con http:// o https:// y tenga un dominio válido.\")\n",
    "    url = input(\"Ingresa el URL del libro de Project Gutenber que quieras analizar: \")\n",
    "print(\"El libro seleccionado se encuentra en:\", url)\n",
    "\n",
    "# Obtenemos el contenido del libro como una cadena de bytes para poder leer los acentos correctamente\n",
    "response = requests.get(url)\n",
    "texto_bytes = response.content\n",
    "# Decodificamos el texto como UTF-8\n",
    "texto = texto_bytes.decode(\"utf-8\")\n",
    "\n",
    "def contar_palabras(texto):\n",
    "    \"\"\"Cuenta las ocurrencias de cada palabra en un texto.\"\"\"\n",
    "    # Convertimos el texto a minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Eliminamos los signos de puntuación\n",
    "    tabla = str.maketrans(\"\", \"\", PUNTUACION)\n",
    "    texto = texto.translate(tabla)\n",
    "    # Dividimos el texto en palabras\n",
    "    palabras = texto.split()\n",
    "    # Creamos un diccionario vacío para almacenar las ocurrencias\n",
    "    ocurrencias = {}\n",
    "    # Recorremos las palabras del texto\n",
    "    for palabra in palabras:\n",
    "        # Incrementamos el contador de la palabra o lo iniciamos en 1\n",
    "        ocurrencias[palabra] = ocurrencias.get(palabra, 0) + 1\n",
    "    return ocurrencias\n",
    "\n",
    "# Contamos las palabras del texto\n",
    "ocurrencias = contar_palabras(texto)\n",
    "# Ordenamos el diccionario de ocurrencias por el valor de las palabras\n",
    "ocurrencias = sorted(ocurrencias.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Obtenemos el conteo de palabras únicas\n",
    "unicas = len(ocurrencias)\n",
    "print(\"Este libro tiene\", unicas, \"palabras únicas\")\n",
    "\n",
    "# Obtenemos las 5 palabras más comunes y su ocurrencia\n",
    "top5 = ocurrencias[:5]\n",
    "print(\"Las 5 palabras más comúnes en el libro son las siguientes:\", top5)\n",
    "\n",
    "# Creamos un conjunto de stopwords en español para quitarlas de la nube\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creamos una nube de palabras a partir del texto\n",
    "nube = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stopwords_es)\n",
    "nube.generate_from_text(texto)\n",
    "\n",
    "# Guardamos la nube de palabras como una imagen\n",
    "nube.to_file(\"nube.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "progra_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
